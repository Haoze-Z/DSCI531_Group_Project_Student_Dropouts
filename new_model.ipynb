{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158623fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_full_shape': (4424, 36),\n",
       " 'X_selected_shape': (4424, 15),\n",
       " 'y_class_distribution': {2: 2209, 0: 1421, 1: 794},\n",
       " 'num_folds': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'data.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Define target and feature sets\n",
    "target_column = 'Target'\n",
    "\n",
    "# Define all features by type\n",
    "categorical_features = [\n",
    "    'Marital status', 'Application mode', 'Application order', 'Course',\n",
    "    'Previous qualification', 'Nacionality', 'Mother\\'s qualification',\n",
    "    'Father\\'s qualification', 'Mother\\'s occupation', 'Father\\'s occupation'\n",
    "]\n",
    "binary_features = [\n",
    "    'Daytime/evening attendance', 'Displaced', 'Educational special needs', 'Debtor',\n",
    "    'Tuition fees up to date', 'Gender', 'Scholarship holder', 'International'\n",
    "]\n",
    "numerical_features = [\n",
    "    'Previous qualification (grade)', 'Admission grade', 'Age at enrollment',\n",
    "    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',\n",
    "    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',\n",
    "    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',\n",
    "    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',\n",
    "    'Unemployment rate', 'Inflation rate', 'GDP'\n",
    "]\n",
    "\n",
    "# Selected features after Random Forest\n",
    "selected_features = {\n",
    "    'categorical': ['Course', 'Application mode'],\n",
    "    'binary': ['Tuition fees up to date', 'Debtor', 'Scholarship holder'],\n",
    "    'numerical': [\n",
    "        'Curricular units 2nd sem (approved)', 'Curricular units 1st sem (approved)',\n",
    "        'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (evaluations)',\n",
    "        'Curricular units 1st sem (evaluations)', 'Curricular units 2nd sem (enrolled)',\n",
    "        'Admission grade', 'Curricular units 1st sem (credited)',\n",
    "        'Previous qualification (grade)', 'Curricular units 1st sem (grade)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Extract full and selected feature lists\n",
    "full_feature_set = categorical_features + binary_features + numerical_features\n",
    "selected_feature_set = (\n",
    "    selected_features['categorical'] +\n",
    "    selected_features['binary'] +\n",
    "    selected_features['numerical']\n",
    ")\n",
    "\n",
    "# Create a version of the dataframe with whitespace-trimmed column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Encode target as categorical (e.g., integer)\n",
    "df[target_column] = df[target_column].astype('category').cat.codes\n",
    "\n",
    "# Prepare to split for CV and feature sets\n",
    "X_full = df[full_feature_set]\n",
    "X_selected = df[selected_feature_set]\n",
    "y = df[target_column]\n",
    "\n",
    "# Set up 10-fold stratified cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Display confirmation\n",
    "{\n",
    "    \"X_full_shape\": X_full.shape,\n",
    "    \"X_selected_shape\": X_selected.shape,\n",
    "    \"y_class_distribution\": y.value_counts().to_dict(),\n",
    "    \"num_folds\": 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7d9904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating feature sets...\n",
      "Running experiments...\n",
      "\n",
      "Running model: DNN\n",
      "  Feature set: full\n",
      "    Fold 1/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "    Fold 2/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "    Fold 3/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "    Fold 4/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "    Fold 5/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "    Fold 6/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "    Fold 7/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "    Fold 8/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "    Fold 9/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "    Fold 10/10\n",
      "      Training baseline model\n",
      "      Training SMOTE model\n",
      "      Training adversarial model\n",
      "  Feature set: selected\n",
      "    Fold 1/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 2/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 3/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 4/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 5/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 6/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 7/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 8/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 9/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 10/10\n",
      "      Training baseline model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training SMOTE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training adversarial model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model: MINN\n",
      "  Feature set: full\n",
      "    Fold 1/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 2/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 3/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 4/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 5/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 6/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 7/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 8/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 9/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 10/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "  Feature set: selected\n",
      "    Fold 1/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 2/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 3/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 4/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 5/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 6/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 7/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 8/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 9/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "    Fold 10/10\n",
      "      Training baseline model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training SMOTE model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "      Training adversarial model\n",
      "Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\n",
      "\n",
      "Running model: TABNET\n",
      "  Feature set: full\n",
      "    Fold 1/10\n",
      "\n",
      "Early stopping occurred at epoch 83 with best_epoch = 73 and best_val_0_accuracy = 0.75169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_accuracy = 0.68849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_accuracy = 0.72235\n",
      "    Fold 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 71 with best_epoch = 61 and best_val_0_accuracy = 0.75169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_accuracy = 0.74718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 60 and best_val_0_accuracy = 0.76524\n",
      "    Fold 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_accuracy = 0.68849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_accuracy = 0.56208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_accuracy = 0.6614\n",
      "    Fold 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 32 and best_val_0_accuracy = 0.70429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_accuracy = 0.6614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 34 and best_val_0_accuracy = 0.73363\n",
      "    Fold 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_accuracy = 0.72624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_accuracy = 0.62896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 67 with best_epoch = 57 and best_val_0_accuracy = 0.78054\n",
      "    Fold 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 47 and best_val_0_accuracy = 0.76244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_accuracy = 0.70362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_accuracy = 0.70588\n",
      "    Fold 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_accuracy = 0.72172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_accuracy = 0.71493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 53 with best_epoch = 43 and best_val_0_accuracy = 0.76697\n",
      "    Fold 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_accuracy = 0.68326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_accuracy = 0.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_accuracy = 0.69457\n",
      "    Fold 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 78 with best_epoch = 68 and best_val_0_accuracy = 0.76471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_accuracy = 0.73077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_accuracy = 0.72624\n",
      "    Fold 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_accuracy = 0.72172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_accuracy = 0.72172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_accuracy = 0.73303\n",
      "  Feature set: selected\n",
      "    Fold 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 60 with best_epoch = 50 and best_val_0_accuracy = 0.73815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_accuracy = 0.70429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_accuracy = 0.73138\n",
      "    Fold 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_accuracy = 0.73138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_accuracy = 0.73589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 51 with best_epoch = 41 and best_val_0_accuracy = 0.77652\n",
      "    Fold 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 45 and best_val_0_accuracy = 0.77878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_accuracy = 0.71558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_accuracy = 0.80813\n",
      "    Fold 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_accuracy = 0.75169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_accuracy = 0.74944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_accuracy = 0.74492\n",
      "    Fold 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_accuracy = 0.73982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_accuracy = 0.72624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_accuracy = 0.77376\n",
      "    Fold 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_accuracy = 0.72398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_accuracy = 0.65837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_accuracy = 0.76697\n",
      "    Fold 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_accuracy = 0.76244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_accuracy = 0.70814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 44 and best_val_0_accuracy = 0.77602\n",
      "    Fold 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_accuracy = 0.70814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_accuracy = 0.70588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_accuracy = 0.75113\n",
      "    Fold 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 51 with best_epoch = 41 and best_val_0_accuracy = 0.73529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_accuracy = 0.72172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_accuracy = 0.77149\n",
      "    Fold 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_accuracy = 0.76244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_accuracy = 0.70814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_accuracy = 0.74661\n",
      "\n",
      "Results Summary:\n",
      "                                fold  accuracy  precision  recall     f1  \\\n",
      "model  feature_set variant                                                 \n",
      "dnn    full        adversarial   4.5     0.738      0.677   0.668  0.671   \n",
      "                   baseline      4.5     0.742      0.683   0.672  0.676   \n",
      "                   smote         4.5     0.736      0.679   0.675  0.677   \n",
      "       selected    adversarial   4.5     0.711      0.644   0.639  0.640   \n",
      "                   baseline      4.5     0.724      0.658   0.651  0.653   \n",
      "                   smote         4.5     0.696      0.635   0.636  0.634   \n",
      "minn   full        adversarial   4.5     0.716      0.655   0.651  0.652   \n",
      "                   baseline      4.5     0.723      0.660   0.653  0.655   \n",
      "                   smote         4.5     0.714      0.651   0.648  0.649   \n",
      "       selected    adversarial   4.5     0.721      0.658   0.653  0.654   \n",
      "                   baseline      4.5     0.726      0.662   0.656  0.658   \n",
      "                   smote         4.5     0.710      0.647   0.647  0.647   \n",
      "tabnet full        adversarial   4.5     0.729      0.652   0.623  0.623   \n",
      "                   baseline      4.5     0.728      0.589   0.603  0.582   \n",
      "                   smote         4.5     0.684      0.623   0.606  0.604   \n",
      "       selected    adversarial   4.5     0.765      0.695   0.661  0.664   \n",
      "                   baseline      4.5     0.743      0.681   0.621  0.607   \n",
      "                   smote         4.5     0.713      0.662   0.652  0.646   \n",
      "\n",
      "                                  SPD     EO  \n",
      "model  feature_set variant                    \n",
      "dnn    full        adversarial -0.213 -0.086  \n",
      "                   baseline    -0.199 -0.061  \n",
      "                   smote       -0.206 -0.084  \n",
      "       selected    adversarial -0.175 -0.048  \n",
      "                   baseline    -0.176 -0.041  \n",
      "                   smote       -0.170 -0.036  \n",
      "minn   full        adversarial -0.207 -0.083  \n",
      "                   baseline    -0.211 -0.060  \n",
      "                   smote       -0.217 -0.080  \n",
      "       selected    adversarial -0.176 -0.026  \n",
      "                   baseline    -0.181 -0.054  \n",
      "                   smote       -0.175 -0.054  \n",
      "tabnet full        adversarial -0.216 -0.084  \n",
      "                   baseline    -0.218 -0.075  \n",
      "                   smote       -0.209 -0.087  \n",
      "       selected    adversarial -0.200 -0.040  \n",
      "                   baseline    -0.232 -0.098  \n",
      "                   smote       -0.210 -0.089  \n",
      "\n",
      "Fairness Metrics Analysis (Gender: 0=female, 1=male):\n",
      "\n",
      "Statistical Parity Difference (SPD):\n",
      "- Negative values indicate bias against females (less likely to be predicted as dropouts)\n",
      "- Positive values indicate bias against males (more likely to be predicted as dropouts)\n",
      "variant             adversarial  baseline  smote\n",
      "model  feature_set                              \n",
      "dnn    full              -0.213    -0.199 -0.206\n",
      "       selected          -0.175    -0.176 -0.170\n",
      "minn   full              -0.207    -0.211 -0.217\n",
      "       selected          -0.176    -0.181 -0.175\n",
      "tabnet full              -0.216    -0.218 -0.209\n",
      "       selected          -0.200    -0.232 -0.210\n",
      "\n",
      "Equalized Opportunity (EO):\n",
      "- Positive values indicate model is better at identifying female dropouts\n",
      "- Negative values indicate model is better at identifying male dropouts\n",
      "variant             adversarial  baseline  smote\n",
      "model  feature_set                              \n",
      "dnn    full              -0.086    -0.061 -0.084\n",
      "       selected          -0.048    -0.041 -0.036\n",
      "minn   full              -0.083    -0.060 -0.080\n",
      "       selected          -0.026    -0.054 -0.054\n",
      "tabnet full              -0.084    -0.075 -0.087\n",
      "       selected          -0.040    -0.098 -0.089\n",
      "\n",
      "Effect of interventions on fairness:\n",
      "\n",
      "DNN - full feature set:\n",
      "  SPD improvement with SMOTE: -0.007 (smaller is better)\n",
      "  SPD improvement with Adversarial: -0.015 (smaller is better)\n",
      "  EO improvement with SMOTE: -0.023 (smaller is better)\n",
      "  EO improvement with Adversarial: -0.025 (smaller is better)\n",
      "\n",
      "DNN - selected feature set:\n",
      "  SPD improvement with SMOTE: 0.006 (smaller is better)\n",
      "  SPD improvement with Adversarial: 0.001 (smaller is better)\n",
      "  EO improvement with SMOTE: 0.005 (smaller is better)\n",
      "  EO improvement with Adversarial: -0.008 (smaller is better)\n",
      "\n",
      "MINN - full feature set:\n",
      "  SPD improvement with SMOTE: -0.006 (smaller is better)\n",
      "  SPD improvement with Adversarial: 0.004 (smaller is better)\n",
      "  EO improvement with SMOTE: -0.02 (smaller is better)\n",
      "  EO improvement with Adversarial: -0.023 (smaller is better)\n",
      "\n",
      "MINN - selected feature set:\n",
      "  SPD improvement with SMOTE: 0.006 (smaller is better)\n",
      "  SPD improvement with Adversarial: 0.004 (smaller is better)\n",
      "  EO improvement with SMOTE: 0.0 (smaller is better)\n",
      "  EO improvement with Adversarial: 0.028 (smaller is better)\n",
      "\n",
      "TABNET - full feature set:\n",
      "  SPD improvement with SMOTE: 0.009 (smaller is better)\n",
      "  SPD improvement with Adversarial: 0.002 (smaller is better)\n",
      "  EO improvement with SMOTE: -0.013 (smaller is better)\n",
      "  EO improvement with Adversarial: -0.01 (smaller is better)\n",
      "\n",
      "TABNET - selected feature set:\n",
      "  SPD improvement with SMOTE: 0.021 (smaller is better)\n",
      "  SPD improvement with Adversarial: 0.032 (smaller is better)\n",
      "  EO improvement with SMOTE: 0.009 (smaller is better)\n",
      "  EO improvement with Adversarial: 0.058 (smaller is better)\n",
      "\n",
      "Effect of feature selection on fairness:\n",
      "\n",
      "DNN - baseline:\n",
      "  SPD improvement with selected features: 0.022 (smaller is better)\n",
      "  EO improvement with selected features: 0.02 (smaller is better)\n",
      "\n",
      "DNN - smote:\n",
      "  SPD improvement with selected features: 0.036 (smaller is better)\n",
      "  EO improvement with selected features: 0.048 (smaller is better)\n",
      "\n",
      "DNN - adversarial:\n",
      "  SPD improvement with selected features: 0.038 (smaller is better)\n",
      "  EO improvement with selected features: 0.038 (smaller is better)\n",
      "\n",
      "MINN - baseline:\n",
      "  SPD improvement with selected features: 0.031 (smaller is better)\n",
      "  EO improvement with selected features: 0.006 (smaller is better)\n",
      "\n",
      "MINN - smote:\n",
      "  SPD improvement with selected features: 0.042 (smaller is better)\n",
      "  EO improvement with selected features: 0.026 (smaller is better)\n",
      "\n",
      "MINN - adversarial:\n",
      "  SPD improvement with selected features: 0.031 (smaller is better)\n",
      "  EO improvement with selected features: 0.057 (smaller is better)\n",
      "\n",
      "TABNET - baseline:\n",
      "  SPD improvement with selected features: -0.013 (smaller is better)\n",
      "  EO improvement with selected features: -0.024 (smaller is better)\n",
      "\n",
      "TABNET - smote:\n",
      "  SPD improvement with selected features: -0.001 (smaller is better)\n",
      "  EO improvement with selected features: -0.002 (smaller is better)\n",
      "\n",
      "TABNET - adversarial:\n",
      "  SPD improvement with selected features: 0.017 (smaller is better)\n",
      "  EO improvement with selected features: 0.044 (smaller is better)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterzhu/DSCI531/.venv/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>feature_set</th>\n",
       "      <th>variant</th>\n",
       "      <th>fold</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>SPD</th>\n",
       "      <th>EO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dnn</td>\n",
       "      <td>full</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>0.767494</td>\n",
       "      <td>0.727213</td>\n",
       "      <td>0.699738</td>\n",
       "      <td>0.709831</td>\n",
       "      <td>-0.151075</td>\n",
       "      <td>-0.065593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dnn</td>\n",
       "      <td>full</td>\n",
       "      <td>smote</td>\n",
       "      <td>0</td>\n",
       "      <td>0.774266</td>\n",
       "      <td>0.737040</td>\n",
       "      <td>0.720192</td>\n",
       "      <td>0.727111</td>\n",
       "      <td>-0.178103</td>\n",
       "      <td>-0.093676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dnn</td>\n",
       "      <td>full</td>\n",
       "      <td>adversarial</td>\n",
       "      <td>0</td>\n",
       "      <td>0.758465</td>\n",
       "      <td>0.722693</td>\n",
       "      <td>0.692861</td>\n",
       "      <td>0.703827</td>\n",
       "      <td>-0.167991</td>\n",
       "      <td>-0.106834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dnn</td>\n",
       "      <td>full</td>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753950</td>\n",
       "      <td>0.694547</td>\n",
       "      <td>0.677786</td>\n",
       "      <td>0.683651</td>\n",
       "      <td>-0.195121</td>\n",
       "      <td>-0.035317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dnn</td>\n",
       "      <td>full</td>\n",
       "      <td>smote</td>\n",
       "      <td>1</td>\n",
       "      <td>0.726862</td>\n",
       "      <td>0.669710</td>\n",
       "      <td>0.666124</td>\n",
       "      <td>0.667784</td>\n",
       "      <td>-0.178195</td>\n",
       "      <td>0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>tabnet</td>\n",
       "      <td>selected</td>\n",
       "      <td>smote</td>\n",
       "      <td>8</td>\n",
       "      <td>0.721719</td>\n",
       "      <td>0.641086</td>\n",
       "      <td>0.623928</td>\n",
       "      <td>0.619968</td>\n",
       "      <td>-0.267447</td>\n",
       "      <td>-0.173930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>tabnet</td>\n",
       "      <td>selected</td>\n",
       "      <td>adversarial</td>\n",
       "      <td>8</td>\n",
       "      <td>0.771493</td>\n",
       "      <td>0.729633</td>\n",
       "      <td>0.671247</td>\n",
       "      <td>0.684426</td>\n",
       "      <td>-0.167438</td>\n",
       "      <td>-0.039602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>tabnet</td>\n",
       "      <td>selected</td>\n",
       "      <td>baseline</td>\n",
       "      <td>9</td>\n",
       "      <td>0.762443</td>\n",
       "      <td>0.724111</td>\n",
       "      <td>0.635844</td>\n",
       "      <td>0.631640</td>\n",
       "      <td>-0.217468</td>\n",
       "      <td>-0.068910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>tabnet</td>\n",
       "      <td>selected</td>\n",
       "      <td>smote</td>\n",
       "      <td>9</td>\n",
       "      <td>0.708145</td>\n",
       "      <td>0.637825</td>\n",
       "      <td>0.636761</td>\n",
       "      <td>0.637074</td>\n",
       "      <td>-0.227363</td>\n",
       "      <td>-0.053285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>tabnet</td>\n",
       "      <td>selected</td>\n",
       "      <td>adversarial</td>\n",
       "      <td>9</td>\n",
       "      <td>0.746606</td>\n",
       "      <td>0.678063</td>\n",
       "      <td>0.646135</td>\n",
       "      <td>0.649563</td>\n",
       "      <td>-0.193692</td>\n",
       "      <td>-0.024840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model feature_set      variant  fold  accuracy  precision    recall  \\\n",
       "0       dnn        full     baseline     0  0.767494   0.727213  0.699738   \n",
       "1       dnn        full        smote     0  0.774266   0.737040  0.720192   \n",
       "2       dnn        full  adversarial     0  0.758465   0.722693  0.692861   \n",
       "3       dnn        full     baseline     1  0.753950   0.694547  0.677786   \n",
       "4       dnn        full        smote     1  0.726862   0.669710  0.666124   \n",
       "..      ...         ...          ...   ...       ...        ...       ...   \n",
       "175  tabnet    selected        smote     8  0.721719   0.641086  0.623928   \n",
       "176  tabnet    selected  adversarial     8  0.771493   0.729633  0.671247   \n",
       "177  tabnet    selected     baseline     9  0.762443   0.724111  0.635844   \n",
       "178  tabnet    selected        smote     9  0.708145   0.637825  0.636761   \n",
       "179  tabnet    selected  adversarial     9  0.746606   0.678063  0.646135   \n",
       "\n",
       "           f1       SPD        EO  \n",
       "0    0.709831 -0.151075 -0.065593  \n",
       "1    0.727111 -0.178103 -0.093676  \n",
       "2    0.703827 -0.167991 -0.106834  \n",
       "3    0.683651 -0.195121 -0.035317  \n",
       "4    0.667784 -0.178195  0.005952  \n",
       "..        ...       ...       ...  \n",
       "175  0.619968 -0.267447 -0.173930  \n",
       "176  0.684426 -0.167438 -0.039602  \n",
       "177  0.631640 -0.217468 -0.068910  \n",
       "178  0.637074 -0.227363 -0.053285  \n",
       "179  0.649563 -0.193692 -0.024840  \n",
       "\n",
       "[180 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "\n",
    "# === LOAD & PREPARE DATA ===\n",
    "def load_data(filepath='data.csv'):\n",
    "    df = pd.read_csv(filepath, delimiter=';')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    target_column = 'Target: String'  # Make sure this matches your actual column name\n",
    "    # Check if target column exists\n",
    "    if target_column not in df.columns:\n",
    "        target_column = 'Target'  # Try alternative name\n",
    "    \n",
    "    df[target_column] = df[target_column].astype('category').cat.codes\n",
    "    y = df[target_column]\n",
    "    return df, y\n",
    "\n",
    "# === DEFINE FEATURE SETS ===\n",
    "categorical_features = [\n",
    "    'Marital status', 'Application mode', 'Application order', 'Course',\n",
    "    'Previous qualification', 'Nacionality', 'Mother\\'s qualification',\n",
    "    'Father\\'s qualification', 'Mother\\'s occupation', 'Father\\'s occupation'\n",
    "]\n",
    "binary_features = [\n",
    "    'Daytime/evening attendance', 'Displaced', 'Educational special needs', 'Debtor',\n",
    "    'Tuition fees up to date', 'Gender', 'Scholarship holder', 'International'\n",
    "]\n",
    "numerical_features = [\n",
    "    'Previous qualification (grade)', 'Admission grade', 'Age at enrollment',\n",
    "    'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',\n",
    "    'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)',\n",
    "    'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)',\n",
    "    'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',\n",
    "    'Unemployment rate', 'Inflation rate', 'GDP'\n",
    "]\n",
    "selected_features = {\n",
    "    'categorical': ['Course', 'Application mode'],\n",
    "    'binary': ['Tuition fees up to date', 'Debtor', 'Scholarship holder'],\n",
    "    'numerical': [\n",
    "        'Curricular units 2nd sem (approved)', 'Curricular units 1st sem (approved)',\n",
    "        'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (evaluations)',\n",
    "        'Curricular units 1st sem (evaluations)', 'Curricular units 2nd sem (enrolled)',\n",
    "        'Admission grade', 'Curricular units 1st sem (credited)',\n",
    "        'Previous qualification (grade)', 'Curricular units 1st sem (grade)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# === FAIRNESS METRICS ===\n",
    "def compute_statistical_parity(y_pred, sensitive_attr, positive_class):\n",
    "    \"\"\"Calculate difference in selection rates between groups.\"\"\"\n",
    "    group_0 = sensitive_attr == 0  # Female\n",
    "    group_1 = sensitive_attr == 1  # Male\n",
    "    if np.sum(group_0) == 0 or np.sum(group_1) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(y_pred[group_0] == positive_class) - np.mean(y_pred[group_1] == positive_class)\n",
    "\n",
    "def compute_equalized_opportunity(y_true, y_pred, sensitive_attr, positive_class):\n",
    "    \"\"\"Calculate difference in true positive rates between groups.\"\"\"\n",
    "    mask_0 = (sensitive_attr == 0) & (y_true == positive_class)  # Female + actual dropouts\n",
    "    mask_1 = (sensitive_attr == 1) & (y_true == positive_class)  # Male + actual dropouts\n",
    "    if np.sum(mask_0) == 0 or np.sum(mask_1) == 0:\n",
    "        return np.nan\n",
    "    tpr_0 = np.mean(y_pred[mask_0] == positive_class)\n",
    "    tpr_1 = np.mean(y_pred[mask_1] == positive_class)\n",
    "    return tpr_0 - tpr_1\n",
    "\n",
    "# === ADVERSARIAL EXAMPLE GENERATOR (SAFER IMPLEMENTATION) ===\n",
    "def generate_adversarial_examples(X, y, model, epsilon=0.01):\n",
    "    \"\"\"Generate adversarial examples with error handling.\"\"\"\n",
    "    try:\n",
    "        X = X.astype(np.float32)\n",
    "        if hasattr(X, \"toarray\"):  # if sparse, convert to dense\n",
    "            X = X.toarray()\n",
    "        \n",
    "        # Simple fitting for different model types\n",
    "        if isinstance(model, MLPClassifier):\n",
    "            # For sklearn models, use fit directly\n",
    "            # Temporarily store early_stopping value\n",
    "            early_stopping = getattr(model, 'early_stopping', False)\n",
    "            if early_stopping:\n",
    "                model.set_params(early_stopping=False)\n",
    "            \n",
    "            model.fit(X, y)\n",
    "            \n",
    "            # Restore original early_stopping value\n",
    "            if early_stopping:\n",
    "                model.set_params(early_stopping=early_stopping)\n",
    "                \n",
    "        elif hasattr(model, 'fit') and not hasattr(model, 'predict_proba'):\n",
    "            # If model can fit but doesn't predict probas\n",
    "            model.fit(X, y)\n",
    "            # Can't create adversarial examples, return original\n",
    "            return X\n",
    "            \n",
    "        # First ensure model can predict probabilities\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            try:\n",
    "                probs = model.predict_proba(X)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in predict_proba: {e}\")\n",
    "                return X  # Return original if predict_proba fails\n",
    "                \n",
    "            grads = np.zeros_like(X)\n",
    "            \n",
    "            # Calculate simple gradient approximation for each feature\n",
    "            for i in range(X.shape[1]):\n",
    "                X_perturbed = X.copy()\n",
    "                X_perturbed[:, i] += epsilon\n",
    "                \n",
    "                # Get new probabilities\n",
    "                try:\n",
    "                    new_probs = model.predict_proba(X_perturbed)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in calculating perturbed probabilities: {e}\")\n",
    "                    continue  # Skip this feature\n",
    "                \n",
    "                # Calculate loss difference - ensuring dimensions match\n",
    "                try:\n",
    "                    # Create one-hot encoding of labels\n",
    "                    one_hot_y = np.zeros_like(probs)\n",
    "                    for j, cls in enumerate(y):\n",
    "                        if cls < one_hot_y.shape[1]:  # Safety check\n",
    "                            one_hot_y[j, cls] = 1\n",
    "                    \n",
    "                    # Calculate loss difference\n",
    "                    loss_diff = np.sum((new_probs - probs) * (one_hot_y - probs), axis=1)\n",
    "                    grads[:, i] = loss_diff\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in gradient calculation: {e}\")\n",
    "                    continue  # Skip this feature\n",
    "            \n",
    "            # Create adversarial examples\n",
    "            if np.any(grads):  # Only perturb if we have gradients\n",
    "                perturbed_X = np.clip(X + epsilon * np.sign(grads), X.min(), X.max())\n",
    "                return perturbed_X\n",
    "            else:\n",
    "                return X  # Return original if no gradients\n",
    "        else:\n",
    "            return X  # Return original if no predict_proba\n",
    "    except Exception as e:\n",
    "        print(f\"Error in adversarial generation: {e}\")\n",
    "        return X  # Return original data on any error\n",
    "\n",
    "# === MODEL FACTORY ===\n",
    "# === MODEL FACTORY with Fixed MINN Implementation ===\n",
    "# === MODEL FACTORY with Fixed MINN Implementation ===\n",
    "def build_model_pipeline(model_type, categorical, numerical):\n",
    "    \"\"\"Create preprocessing pipeline with specified model.\"\"\"\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical),\n",
    "        ('num', StandardScaler(), numerical)\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    if model_type == \"dnn\":\n",
    "        # Simple Dense Neural Network using sklearn's MLPClassifier\n",
    "        classifier = MLPClassifier(\n",
    "            hidden_layer_sizes=(64,),\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            solver='adam',\n",
    "            learning_rate_init=1e-3,\n",
    "            activation='tanh',\n",
    "            early_stopping=False,\n",
    "            tol=1e-4\n",
    "        )\n",
    "    elif model_type == \"minn\":\n",
    "        # Simpler implementation using MLPClassifier for MINN\n",
    "        # This avoids the 'super' object has no attribute '__sklearn_tags__' error\n",
    "        classifier = MLPClassifier(\n",
    "            hidden_layer_sizes=(64, 32),  # Two hidden layers to mimic MINN\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            solver='adam',\n",
    "            learning_rate_init=1e-3,\n",
    "            activation='relu',\n",
    "            early_stopping=False\n",
    "        )\n",
    "        print(\"Using MLPClassifier as a direct substitute for MINN to avoid compatibility issues\")\n",
    "            \n",
    "    elif model_type == \"tabnet\":\n",
    "        # Using TabNet classifier if available\n",
    "        try:\n",
    "            from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "            \n",
    "            classifier = TabNetClassifier(\n",
    "                optimizer_params=dict(lr=2e-2),\n",
    "                verbose=0\n",
    "            )\n",
    "        except ImportError as e:\n",
    "            print(f\"TabNet import error: {e}. Falling back to MLPClassifier.\")\n",
    "            # Fallback if TabNet is not available\n",
    "            classifier = MLPClassifier(\n",
    "                hidden_layer_sizes=(32,),\n",
    "                max_iter=500,\n",
    "                random_state=42,\n",
    "                early_stopping=False\n",
    "            )\n",
    "    else:\n",
    "        # Default fallback\n",
    "        classifier = MLPClassifier(\n",
    "            hidden_layer_sizes=(32,),\n",
    "            max_iter=500,\n",
    "            random_state=42,\n",
    "            early_stopping=False\n",
    "        )\n",
    "\n",
    "    return Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "\n",
    "  \n",
    "\n",
    "# === EXPERIMENT WRAPPER ===\n",
    "def run_full_experiment(X_full, X_selected, y, df, skf, selected_features, model_types=[\"dnn\", \"minn\", \"tabnet\"]):\n",
    "    \"\"\"Run cross-validation experiment with specified models and feature sets.\"\"\"\n",
    "    results = defaultdict(list)\n",
    "    feature_sets = {\"full\": X_full, \"selected\": X_selected}\n",
    "\n",
    "    for model_type in model_types:\n",
    "        print(f\"\\nRunning model: {model_type.upper()}\")\n",
    "        for feature_set_name, X in feature_sets.items():\n",
    "            print(f\"  Feature set: {feature_set_name}\")\n",
    "            if feature_set_name == 'full':\n",
    "                categorical = [f for f in categorical_features if f in X.columns]\n",
    "                numerical = [f for f in numerical_features if f in X.columns]\n",
    "                binary = [f for f in binary_features if f in X.columns]\n",
    "            else:\n",
    "                categorical = [f for f in selected_features['categorical'] if f in X.columns]\n",
    "                numerical = [f for f in selected_features['numerical'] if f in X.columns]\n",
    "                binary = [f for f in selected_features['binary'] if f in X.columns]\n",
    "\n",
    "            # Merge binary features into numerical for preprocessing\n",
    "            all_numerical = numerical + binary\n",
    "\n",
    "            for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "                print(f\"    Fold {fold_idx + 1}/10\")\n",
    "                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "                \n",
    "                # Get sensitive attribute\n",
    "                gender_column = 'Gender'\n",
    "                if gender_column in df.columns:\n",
    "                    gender_test = df.iloc[test_idx][gender_column].values\n",
    "                else:\n",
    "                    # Use placeholder if gender not available\n",
    "                    gender_test = np.zeros(len(test_idx))\n",
    "                \n",
    "                y_true = y_test.to_numpy()\n",
    "                positive_class = 0  # assuming 0 = Dropout\n",
    "\n",
    "                def evaluate_and_store(model_name, variant, y_pred):\n",
    "                    \"\"\"Store evaluation metrics.\"\"\"\n",
    "                    results[\"model\"].append(model_name)\n",
    "                    results[\"feature_set\"].append(feature_set_name)\n",
    "                    results[\"variant\"].append(variant)\n",
    "                    results[\"fold\"].append(fold_idx)\n",
    "                    \n",
    "                    # Performance metrics\n",
    "                    results[\"accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "                    results[\"precision\"].append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "                    results[\"recall\"].append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "                    results[\"f1\"].append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "                    \n",
    "                    # Fairness metrics\n",
    "                    results[\"SPD\"].append(compute_statistical_parity(y_pred, gender_test, positive_class))\n",
    "                    results[\"EO\"].append(compute_equalized_opportunity(y_true, y_pred, gender_test, positive_class))\n",
    "\n",
    "                try:\n",
    "                    # Special TabNet handling\n",
    "                    if model_type == \"tabnet\":\n",
    "                        try:\n",
    "                            from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "                            \n",
    "                            # Preprocess data for TabNet\n",
    "                            preprocessor = ColumnTransformer([\n",
    "                                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical),\n",
    "                                ('num', StandardScaler(), all_numerical)\n",
    "                            ], remainder='drop')\n",
    "                            \n",
    "                            # Prepare training data\n",
    "                            X_train_enc = preprocessor.fit_transform(X_train)\n",
    "                            X_test_enc = preprocessor.transform(X_test)\n",
    "                            \n",
    "                            # Baseline TabNet\n",
    "                            tabnet_model = TabNetClassifier(\n",
    "                                optimizer_params=dict(lr=2e-2),\n",
    "                                verbose=0\n",
    "                            )\n",
    "                            tabnet_model.fit(\n",
    "                                X_train=X_train_enc, y_train=y_train.values,\n",
    "                                eval_set=[(X_test_enc, y_test.values)],\n",
    "                                max_epochs=100\n",
    "                            )\n",
    "                            y_pred = tabnet_model.predict(X_test_enc)\n",
    "                            evaluate_and_store(model_type, \"baseline\", y_pred)\n",
    "                            \n",
    "                            # SMOTE with TabNet\n",
    "                            try:\n",
    "                                sm = SMOTE(random_state=42)\n",
    "                                X_train_res, y_train_res = sm.fit_resample(X_train_enc, y_train)\n",
    "                                \n",
    "                                tabnet_smote = TabNetClassifier(\n",
    "                                    optimizer_params=dict(lr=2e-2),\n",
    "                                    verbose=0\n",
    "                                )\n",
    "                                tabnet_smote.fit(\n",
    "                                    X_train=X_train_res, y_train=y_train_res.values,\n",
    "                                    eval_set=[(X_test_enc, y_test.values)],\n",
    "                                    max_epochs=100\n",
    "                                )\n",
    "                                y_pred_smote = tabnet_smote.predict(X_test_enc)\n",
    "                                evaluate_and_store(model_type, \"smote\", y_pred_smote)\n",
    "                            except Exception as e:\n",
    "                                print(f\"      TabNet SMOTE error: {e}\")\n",
    "                                evaluate_and_store(model_type, \"smote\", y_pred)\n",
    "                            \n",
    "                            # Adversarial with TabNet - simplified for TabNet\n",
    "                            try:\n",
    "                                # Generate simple adversarial examples with noise\n",
    "                                np.random.seed(42)\n",
    "                                noise = np.random.normal(0, 0.01, X_train_enc.shape)\n",
    "                                adv_X = np.clip(X_train_enc + noise, X_train_enc.min(), X_train_enc.max())\n",
    "                                \n",
    "                                # Combine original and adversarial\n",
    "                                combined_X = np.vstack([X_train_enc, adv_X])\n",
    "                                combined_y = np.concatenate([y_train.values, y_train.values])\n",
    "                                \n",
    "                                tabnet_adv = TabNetClassifier(\n",
    "                                    optimizer_params=dict(lr=2e-2),\n",
    "                                    verbose=0\n",
    "                                )\n",
    "                                tabnet_adv.fit(\n",
    "                                    X_train=combined_X, y_train=combined_y,\n",
    "                                    eval_set=[(X_test_enc, y_test.values)],\n",
    "                                    max_epochs=100\n",
    "                                )\n",
    "                                y_pred_adv = tabnet_adv.predict(X_test_enc)\n",
    "                                evaluate_and_store(model_type, \"adversarial\", y_pred_adv)\n",
    "                            except Exception as e:\n",
    "                                print(f\"      TabNet adversarial error: {e}\")\n",
    "                                evaluate_and_store(model_type, \"adversarial\", y_pred)\n",
    "                                \n",
    "                        except ImportError as e:\n",
    "                            print(f\"TabNet not available: {e}. Skipping this model type.\")\n",
    "                            # Fill with NaN in case TabNet is not available\n",
    "                            evaluate_and_store(model_type, \"baseline\", np.full_like(y_test.values, np.nan))\n",
    "                            evaluate_and_store(model_type, \"smote\", np.full_like(y_test.values, np.nan))\n",
    "                            evaluate_and_store(model_type, \"adversarial\", np.full_like(y_test.values, np.nan))\n",
    "                            continue\n",
    "                            \n",
    "                    else:\n",
    "                        # Standard scikit-learn pipeline for DNN and MINN\n",
    "                        # Baseline\n",
    "                        print(f\"      Training baseline model\")\n",
    "                        pipe_base = build_model_pipeline(model_type, categorical, all_numerical)\n",
    "                        pipe_base.fit(X_train, y_train)\n",
    "                        y_pred = pipe_base.predict(X_test)\n",
    "                        evaluate_and_store(model_type, \"baseline\", y_pred)\n",
    "\n",
    "                        # SMOTE\n",
    "                        print(f\"      Training SMOTE model\")\n",
    "                        pipe_smote = build_model_pipeline(model_type, categorical, all_numerical)\n",
    "                        X_train_enc = pipe_smote.named_steps['preprocessor'].fit_transform(X_train)\n",
    "                        \n",
    "                        # Ensure X_train_enc is dense for SMOTE\n",
    "                        if hasattr(X_train_enc, \"toarray\"):\n",
    "                            X_train_enc = X_train_enc.toarray()\n",
    "                        \n",
    "                        # Apply SMOTE\n",
    "                        try:\n",
    "                            sm = SMOTE(random_state=42)\n",
    "                            X_train_res, y_train_res = sm.fit_resample(X_train_enc, y_train)\n",
    "                            \n",
    "                            # Special handling for MINN with SMOTE\n",
    "                            if model_type == \"minn\" and \"KerasClassifier\" in str(type(pipe_smote.named_steps['clf'])):\n",
    "                                # Manually update input dimensions for Keras\n",
    "                                clf = pipe_smote.named_steps['clf']\n",
    "                                # Use scikit-learn mechanism to update model parameters\n",
    "                                clf.set_params(model__meta={\"n_features_in_\": X_train_res.shape[1], \"n_classes_\": len(np.unique(y))})\n",
    "                            \n",
    "                            pipe_smote.named_steps['clf'].fit(X_train_res, y_train_res)\n",
    "                            X_test_enc = pipe_smote.named_steps['preprocessor'].transform(X_test)\n",
    "                            \n",
    "                            if hasattr(X_test_enc, \"toarray\"):\n",
    "                                X_test_enc = X_test_enc.toarray()\n",
    "                                \n",
    "                            y_pred_smote = pipe_smote.named_steps['clf'].predict(X_test_enc)\n",
    "                            evaluate_and_store(model_type, \"smote\", y_pred_smote)\n",
    "                        except Exception as e:\n",
    "                            print(f\"      SMOTE error: {e}\")\n",
    "                            # If SMOTE fails, use baseline predictions\n",
    "                            evaluate_and_store(model_type, \"smote\", y_pred)\n",
    "\n",
    "                        # Adversarial\n",
    "                        print(f\"      Training adversarial model\")\n",
    "                        pipe_adv = build_model_pipeline(model_type, categorical, all_numerical)\n",
    "                        X_train_enc = pipe_adv.named_steps['preprocessor'].fit_transform(X_train)\n",
    "                        \n",
    "                        # Ensure X_train_enc is dense\n",
    "                        if hasattr(X_train_enc, \"toarray\"):\n",
    "                            X_train_enc = X_train_enc.toarray()\n",
    "                        \n",
    "                        try:\n",
    "                            # Generate adversarial examples\n",
    "                            adv_X = generate_adversarial_examples(X_train_enc, y_train.to_numpy(), \n",
    "                                                                pipe_adv.named_steps['clf'])\n",
    "                            \n",
    "                            # Concatenate original and adversarial examples\n",
    "                            combined_X = np.vstack([X_train_enc, adv_X])\n",
    "                            combined_y = np.concatenate([y_train.to_numpy(), y_train.to_numpy()])\n",
    "                            \n",
    "                            # Train on combined data\n",
    "                            pipe_adv.named_steps['clf'].fit(combined_X, combined_y)\n",
    "                            \n",
    "                            # Transform test data\n",
    "                            X_test_enc = pipe_adv.named_steps['preprocessor'].transform(X_test)\n",
    "                            if hasattr(X_test_enc, \"toarray\"):\n",
    "                                X_test_enc = X_test_enc.toarray()\n",
    "                            \n",
    "                            # Predict\n",
    "                            y_pred_adv = pipe_adv.named_steps['clf'].predict(X_test_enc)\n",
    "                            evaluate_and_store(model_type, \"adversarial\", y_pred_adv)\n",
    "                        except Exception as e:\n",
    "                            print(f\"      Adversarial error: {e}\")\n",
    "                            # If adversarial fails, use baseline predictions\n",
    "                            evaluate_and_store(model_type, \"adversarial\", y_pred)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in fold {fold_idx} for {model_type} on {feature_set_name}: {e}\")\n",
    "                    # Fill with NaN in case of complete failure\n",
    "                    evaluate_and_store(model_type, \"baseline\", np.full_like(y_test, np.nan))\n",
    "                    evaluate_and_store(model_type, \"smote\", np.full_like(y_test, np.nan))\n",
    "                    evaluate_and_store(model_type, \"adversarial\", np.full_like(y_test, np.nan))\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# === MAIN EXECUTION BLOCK ===\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    df, y = load_data()\n",
    "    \n",
    "    # Create feature sets\n",
    "    print(\"Creating feature sets...\")\n",
    "    full_feature_set = categorical_features + binary_features + numerical_features\n",
    "    selected_feature_set = (\n",
    "        selected_features['categorical'] +\n",
    "        selected_features['binary'] +\n",
    "        selected_features['numerical']\n",
    "    )\n",
    "    \n",
    "    # Check for missing columns in the dataset\n",
    "    missing_full = [col for col in full_feature_set if col not in df.columns]\n",
    "    if missing_full:\n",
    "        print(f\"Warning: Missing columns in full feature set: {missing_full}\")\n",
    "        full_feature_set = [col for col in full_feature_set if col in df.columns]\n",
    "        \n",
    "    missing_selected = [col for col in selected_feature_set if col not in df.columns]\n",
    "    if missing_selected:\n",
    "        print(f\"Warning: Missing columns in selected feature set: {missing_selected}\")\n",
    "        selected_feature_set = [col for col in selected_feature_set if col in df.columns]\n",
    "    \n",
    "    X_full = df[full_feature_set]\n",
    "    X_selected = df[selected_feature_set]\n",
    "    \n",
    "    # Setup cross-validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  # Back to 10 folds as in original\n",
    "    \n",
    "    # Run the experiment with all three models\n",
    "    print(\"Running experiments...\")\n",
    "    df_results = run_full_experiment(X_full, X_selected, y, df, skf, selected_features, \n",
    "                                    model_types=[\"dnn\", \"minn\", \"tabnet\"])\n",
    "    \n",
    "    # Display results with focus on fairness metrics\n",
    "    print(\"\\nResults Summary:\")\n",
    "    summary = df_results.groupby(['model', 'feature_set', 'variant']).mean().round(3)\n",
    "    print(summary)\n",
    "    \n",
    "    # Print focused fairness metrics analysis\n",
    "    print(\"\\nFairness Metrics Analysis (Gender: 0=female, 1=male):\")\n",
    "    fairness_metrics = ['SPD', 'EO']\n",
    "    fairness_summary = summary[fairness_metrics]\n",
    "    \n",
    "    print(\"\\nStatistical Parity Difference (SPD):\")\n",
    "    print(\"- Negative values indicate bias against females (less likely to be predicted as dropouts)\")\n",
    "    print(\"- Positive values indicate bias against males (more likely to be predicted as dropouts)\")\n",
    "    print(fairness_summary['SPD'].unstack().round(3))\n",
    "    \n",
    "    print(\"\\nEqualized Opportunity (EO):\")\n",
    "    print(\"- Positive values indicate model is better at identifying female dropouts\")\n",
    "    print(\"- Negative values indicate model is better at identifying male dropouts\")\n",
    "    print(fairness_summary['EO'].unstack().round(3))\n",
    "    \n",
    "    # Check if the interventions (SMOTE and adversarial) improve fairness\n",
    "    print(\"\\nEffect of interventions on fairness:\")\n",
    "    for model in df_results['model'].unique():\n",
    "        for feature_set in df_results['feature_set'].unique():\n",
    "            baseline = df_results[(df_results['model'] == model) & \n",
    "                                  (df_results['feature_set'] == feature_set) & \n",
    "                                  (df_results['variant'] == 'baseline')][fairness_metrics].mean().abs()\n",
    "            \n",
    "            smote = df_results[(df_results['model'] == model) & \n",
    "                              (df_results['feature_set'] == feature_set) & \n",
    "                              (df_results['variant'] == 'smote')][fairness_metrics].mean().abs()\n",
    "            \n",
    "            adv = df_results[(df_results['model'] == model) & \n",
    "                            (df_results['feature_set'] == feature_set) & \n",
    "                            (df_results['variant'] == 'adversarial')][fairness_metrics].mean().abs()\n",
    "            \n",
    "            print(f\"\\n{model.upper()} - {feature_set} feature set:\")\n",
    "            print(f\"  SPD improvement with SMOTE: {(baseline['SPD'] - smote['SPD']).round(3)} (smaller is better)\")\n",
    "            print(f\"  SPD improvement with Adversarial: {(baseline['SPD'] - adv['SPD']).round(3)} (smaller is better)\")\n",
    "            print(f\"  EO improvement with SMOTE: {(baseline['EO'] - smote['EO']).round(3)} (smaller is better)\")\n",
    "            print(f\"  EO improvement with Adversarial: {(baseline['EO'] - adv['EO']).round(3)} (smaller is better)\")\n",
    "    \n",
    "    # Compare full vs selected features impact on fairness\n",
    "    print(\"\\nEffect of feature selection on fairness:\")\n",
    "    for model in df_results['model'].unique():\n",
    "        for variant in df_results['variant'].unique():\n",
    "            full = df_results[(df_results['model'] == model) & \n",
    "                             (df_results['feature_set'] == 'full') & \n",
    "                             (df_results['variant'] == variant)][fairness_metrics].mean().abs()\n",
    "            \n",
    "            selected = df_results[(df_results['model'] == model) & \n",
    "                                 (df_results['feature_set'] == 'selected') & \n",
    "                                 (df_results['variant'] == variant)][fairness_metrics].mean().abs()\n",
    "            \n",
    "            print(f\"\\n{model.upper()} - {variant}:\")\n",
    "            print(f\"  SPD improvement with selected features: {(full['SPD'] - selected['SPD']).round(3)} (smaller is better)\")\n",
    "            print(f\"  EO improvement with selected features: {(full['EO'] - selected['EO']).round(3)} (smaller is better)\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f462e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee1f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
